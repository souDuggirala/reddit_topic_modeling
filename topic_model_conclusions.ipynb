{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ElementTree XML API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimentation with Python's ElementTree API to be able to use it to look at the diagnostic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse('ladvice_min3000/ladvice_min3000_numtopics5/ladvice_min3000_numtopics5_optinterval50/ladvice_diagnostics_min3000_numtopics5_optinterval50.xml')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'model'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root.tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic {'id': '0', 'tokens': '960699.0000', 'document_entropy': '8.5349', 'word-length': '5.2000', 'coherence': '-1869.5797', 'uniform_dist': '3.5594', 'corpus_dist': '0.6506', 'eff_num_words': '481.2395', 'token-doc-diff': '0.0309', 'rank_1_docs': '0.2227', 'allocation_ratio': '0.2451', 'allocation_count': '0.2789', 'exclusivity': '0.5471'}\n",
      "topic {'id': '1', 'tokens': '1276123.0000', 'document_entropy': '8.7212', 'word-length': '5.4200', 'coherence': '-1588.1932', 'uniform_dist': '3.5622', 'corpus_dist': '0.5469', 'eff_num_words': '632.4838', 'token-doc-diff': '0.0139', 'rank_1_docs': '0.2378', 'allocation_ratio': '0.2508', 'allocation_count': '0.3069', 'exclusivity': '0.6016'}\n",
      "topic {'id': '2', 'tokens': '1865544.0000', 'document_entropy': '9.1056', 'word-length': '5.4200', 'coherence': '-1619.8216', 'uniform_dist': '3.5159', 'corpus_dist': '0.3941', 'eff_num_words': '675.3960', 'token-doc-diff': '0.0096', 'rank_1_docs': '0.3173', 'allocation_ratio': '0.3172', 'allocation_count': '0.4154', 'exclusivity': '0.4679'}\n",
      "topic {'id': '3', 'tokens': '1808891.0000', 'document_entropy': '8.9693', 'word-length': '5.5200', 'coherence': '-1504.7021', 'uniform_dist': '3.1182', 'corpus_dist': '0.4192', 'eff_num_words': '981.0320', 'token-doc-diff': '0.0084', 'rank_1_docs': '0.2554', 'allocation_ratio': '0.2521', 'allocation_count': '0.3553', 'exclusivity': '0.3796'}\n",
      "topic {'id': '4', 'tokens': '1043088.0000', 'document_entropy': '8.4880', 'word-length': '5.4400', 'coherence': '-1498.0558', 'uniform_dist': '3.7949', 'corpus_dist': '0.5957', 'eff_num_words': '423.5518', 'token-doc-diff': '0.0231', 'rank_1_docs': '0.3007', 'allocation_ratio': '0.3617', 'allocation_count': '0.3496', 'exclusivity': '0.5645'}\n"
     ]
    }
   ],
   "source": [
    "for child in root:\n",
    "    print(child.tag, child.attrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic {'id': '0', 'tokens': '960699.0000', 'document_entropy': '8.5349', 'word-length': '5.2000', 'coherence': '-1869.5797', 'uniform_dist': '3.5594', 'corpus_dist': '0.6506', 'eff_num_words': '481.2395', 'token-doc-diff': '0.0309', 'rank_1_docs': '0.2227', 'allocation_ratio': '0.2451', 'allocation_count': '0.2789', 'exclusivity': '0.5471'}\n"
     ]
    }
   ],
   "source": [
    "print(root[0].tag, root[0].attrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word {'rank': '1', 'count': '22666', 'prob': '0.02359', 'cumulative': '0.02359', 'docs': '3747', 'word-length': '3.0000', 'coherence': '0.0000', 'uniform_dist': '0.1762', 'corpus_dist': '0.0458', 'token-doc-diff': '0.0064', 'exclusivity': '0.9749'}\n",
      "word {'rank': '2', 'count': '18562', 'prob': '0.01932', 'cumulative': '0.04291', 'docs': '3059', 'word-length': '9.0000', 'coherence': '-0.8547', 'uniform_dist': '0.1405', 'corpus_dist': '0.0347', 'token-doc-diff': '0.0053', 'exclusivity': '0.8814'}\n",
      "word {'rank': '3', 'count': '10441', 'prob': '0.01087', 'cumulative': '0.05378', 'docs': '1061', 'word-length': '3.0000', 'coherence': '-2.6794', 'uniform_dist': '0.0728', 'corpus_dist': '0.0213', 'token-doc-diff': '0.0058', 'exclusivity': '0.9792'}\n",
      "word {'rank': '4', 'count': '6702', 'prob': '0.00698', 'cumulative': '0.06076', 'docs': '1962', 'word-length': '8.0000', 'coherence': '-1.5516', 'uniform_dist': '0.0436', 'corpus_dist': '0.0069', 'token-doc-diff': '0.0002', 'exclusivity': '0.4101'}\n",
      "word {'rank': '5', 'count': '6032', 'prob': '0.00628', 'cumulative': '0.06704', 'docs': '1838', 'word-length': '7.0000', 'coherence': '-2.6354', 'uniform_dist': '0.0386', 'corpus_dist': '0.0124', 'token-doc-diff': '0.0002', 'exclusivity': '1.0000'}\n",
      "word {'rank': '6', 'count': '5646', 'prob': '0.00588', 'cumulative': '0.07291', 'docs': '2922', 'word-length': '4.0000', 'coherence': '-0.9176', 'uniform_dist': '0.0357', 'corpus_dist': '0.0017', 'token-doc-diff': '0.0004', 'exclusivity': '0.2587'}\n",
      "word {'rank': '7', 'count': '4925', 'prob': '0.00513', 'cumulative': '0.07804', 'docs': '2818', 'word-length': '4.0000', 'coherence': '-0.9031', 'uniform_dist': '0.0305', 'corpus_dist': '-0.0012', 'token-doc-diff': '0.0006', 'exclusivity': '0.1614'}\n",
      "word {'rank': '8', 'count': '4775', 'prob': '0.00497', 'cumulative': '0.08301', 'docs': '1941', 'word-length': '7.0000', 'coherence': '-1.7517', 'uniform_dist': '0.0294', 'corpus_dist': '0.0016', 'token-doc-diff': '0.0000', 'exclusivity': '0.3028'}\n",
      "word {'rank': '9', 'count': '4450', 'prob': '0.00463', 'cumulative': '0.08764', 'docs': '2254', 'word-length': '3.0000', 'coherence': '-1.2765', 'uniform_dist': '0.0271', 'corpus_dist': '-0.0003', 'token-doc-diff': '0.0003', 'exclusivity': '0.1926'}\n",
      "word {'rank': '10', 'count': '4390', 'prob': '0.00457', 'cumulative': '0.09221', 'docs': '1405', 'word-length': '6.0000', 'coherence': '-3.1155', 'uniform_dist': '0.0266', 'corpus_dist': '0.0089', 'token-doc-diff': '0.0001', 'exclusivity': '0.9849'}\n",
      "word {'rank': '11', 'count': '4282', 'prob': '0.00446', 'cumulative': '0.09667', 'docs': '1727', 'word-length': '6.0000', 'coherence': '-1.6835', 'uniform_dist': '0.0259', 'corpus_dist': '0.0067', 'token-doc-diff': '0.0000', 'exclusivity': '0.6567'}\n",
      "word {'rank': '12', 'count': '4055', 'prob': '0.00422', 'cumulative': '0.10089', 'docs': '1603', 'word-length': '7.0000', 'coherence': '-2.5595', 'uniform_dist': '0.0243', 'corpus_dist': '0.0079', 'token-doc-diff': '0.0000', 'exclusivity': '0.9353'}\n",
      "word {'rank': '13', 'count': '3852', 'prob': '0.00401', 'cumulative': '0.10490', 'docs': '943', 'word-length': '6.0000', 'coherence': '-3.0339', 'uniform_dist': '0.0228', 'corpus_dist': '0.0075', 'token-doc-diff': '0.0004', 'exclusivity': '0.9480'}\n",
      "word {'rank': '14', 'count': '3845', 'prob': '0.00400', 'cumulative': '0.10890', 'docs': '1409', 'word-length': '8.0000', 'coherence': '-2.7764', 'uniform_dist': '0.0228', 'corpus_dist': '0.0078', 'token-doc-diff': '0.0000', 'exclusivity': '0.9728'}\n",
      "word {'rank': '15', 'count': '3560', 'prob': '0.00371', 'cumulative': '0.11261', 'docs': '1644', 'word-length': '6.0000', 'coherence': '-1.3871', 'uniform_dist': '0.0208', 'corpus_dist': '0.0005', 'token-doc-diff': '0.0001', 'exclusivity': '0.2583'}\n",
      "word {'rank': '16', 'count': '3534', 'prob': '0.00368', 'cumulative': '0.11629', 'docs': '2182', 'word-length': '4.0000', 'coherence': '-1.1180', 'uniform_dist': '0.0206', 'corpus_dist': '-0.0006', 'token-doc-diff': '0.0007', 'exclusivity': '0.1722'}\n",
      "word {'rank': '17', 'count': '3481', 'prob': '0.00362', 'cumulative': '0.11991', 'docs': '1401', 'word-length': '5.0000', 'coherence': '-1.9830', 'uniform_dist': '0.0203', 'corpus_dist': '0.0060', 'token-doc-diff': '0.0000', 'exclusivity': '0.8059'}\n",
      "word {'rank': '18', 'count': '3463', 'prob': '0.00360', 'cumulative': '0.12352', 'docs': '2021', 'word-length': '4.0000', 'coherence': '-1.2921', 'uniform_dist': '0.0202', 'corpus_dist': '0.0007', 'token-doc-diff': '0.0005', 'exclusivity': '0.2477'}\n",
      "word {'rank': '19', 'count': '3323', 'prob': '0.00346', 'cumulative': '0.12697', 'docs': '1738', 'word-length': '5.0000', 'coherence': '-1.7013', 'uniform_dist': '0.0192', 'corpus_dist': '0.0029', 'token-doc-diff': '0.0002', 'exclusivity': '0.4494'}\n",
      "word {'rank': '20', 'count': '3295', 'prob': '0.00343', 'cumulative': '0.13040', 'docs': '832', 'word-length': '8.0000', 'coherence': '-3.1197', 'uniform_dist': '0.0190', 'corpus_dist': '0.0059', 'token-doc-diff': '0.0003', 'exclusivity': '0.7987'}\n",
      "word {'rank': '21', 'count': '3264', 'prob': '0.00340', 'cumulative': '0.13380', 'docs': '1339', 'word-length': '5.0000', 'coherence': '-2.1664', 'uniform_dist': '0.0188', 'corpus_dist': '0.0037', 'token-doc-diff': '0.0000', 'exclusivity': '0.4858'}\n",
      "word {'rank': '22', 'count': '3046', 'prob': '0.00317', 'cumulative': '0.13697', 'docs': '1920', 'word-length': '4.0000', 'coherence': '-1.2564', 'uniform_dist': '0.0173', 'corpus_dist': '-0.0003', 'token-doc-diff': '0.0006', 'exclusivity': '0.1931'}\n",
      "word {'rank': '23', 'count': '3044', 'prob': '0.00317', 'cumulative': '0.14014', 'docs': '835', 'word-length': '7.0000', 'coherence': '-2.9585', 'uniform_dist': '0.0173', 'corpus_dist': '0.0060', 'token-doc-diff': '0.0002', 'exclusivity': '0.9390'}\n",
      "word {'rank': '24', 'count': '3042', 'prob': '0.00317', 'cumulative': '0.14331', 'docs': '1777', 'word-length': '3.0000', 'coherence': '-1.3893', 'uniform_dist': '0.0173', 'corpus_dist': '0.0017', 'token-doc-diff': '0.0004', 'exclusivity': '0.3264'}\n",
      "word {'rank': '25', 'count': '2948', 'prob': '0.00307', 'cumulative': '0.14638', 'docs': '645', 'word-length': '4.0000', 'coherence': '-4.0671', 'uniform_dist': '0.0167', 'corpus_dist': '0.0059', 'token-doc-diff': '0.0004', 'exclusivity': '0.9508'}\n",
      "word {'rank': '26', 'count': '2934', 'prob': '0.00305', 'cumulative': '0.14943', 'docs': '1569', 'word-length': '5.0000', 'coherence': '-1.6101', 'uniform_dist': '0.0166', 'corpus_dist': '-0.0006', 'token-doc-diff': '0.0003', 'exclusivity': '0.1632'}\n",
      "word {'rank': '27', 'count': '2856', 'prob': '0.00297', 'cumulative': '0.15240', 'docs': '1441', 'word-length': '5.0000', 'coherence': '-2.2660', 'uniform_dist': '0.0160', 'corpus_dist': '0.0050', 'token-doc-diff': '0.0002', 'exclusivity': '0.8346'}\n",
      "word {'rank': '28', 'count': '2785', 'prob': '0.00290', 'cumulative': '0.15530', 'docs': '1042', 'word-length': '7.0000', 'coherence': '-2.7714', 'uniform_dist': '0.0156', 'corpus_dist': '0.0040', 'token-doc-diff': '0.0000', 'exclusivity': '0.6875'}\n",
      "word {'rank': '29', 'count': '2751', 'prob': '0.00286', 'cumulative': '0.15817', 'docs': '1842', 'word-length': '4.0000', 'coherence': '-1.2969', 'uniform_dist': '0.0154', 'corpus_dist': '-0.0001', 'token-doc-diff': '0.0007', 'exclusivity': '0.1959'}\n",
      "word {'rank': '30', 'count': '2631', 'prob': '0.00274', 'cumulative': '0.16090', 'docs': '594', 'word-length': '6.0000', 'coherence': '-4.4108', 'uniform_dist': '0.0146', 'corpus_dist': '0.0053', 'token-doc-diff': '0.0003', 'exclusivity': '0.9696'}\n",
      "word {'rank': '31', 'count': '2587', 'prob': '0.00269', 'cumulative': '0.16360', 'docs': '1640', 'word-length': '6.0000', 'coherence': '-1.4507', 'uniform_dist': '0.0143', 'corpus_dist': '-0.0010', 'token-doc-diff': '0.0005', 'exclusivity': '0.1508'}\n",
      "word {'rank': '32', 'count': '2586', 'prob': '0.00269', 'cumulative': '0.16629', 'docs': '1687', 'word-length': '4.0000', 'coherence': '-1.4493', 'uniform_dist': '0.0143', 'corpus_dist': '-0.0005', 'token-doc-diff': '0.0006', 'exclusivity': '0.1701'}\n",
      "word {'rank': '33', 'count': '2542', 'prob': '0.00265', 'cumulative': '0.16893', 'docs': '1255', 'word-length': '7.0000', 'coherence': '-2.1036', 'uniform_dist': '0.0140', 'corpus_dist': '0.0025', 'token-doc-diff': '0.0001', 'exclusivity': '0.4252'}\n",
      "word {'rank': '34', 'count': '2499', 'prob': '0.00260', 'cumulative': '0.17154', 'docs': '1320', 'word-length': '6.0000', 'coherence': '-1.8118', 'uniform_dist': '0.0137', 'corpus_dist': '0.0010', 'token-doc-diff': '0.0002', 'exclusivity': '0.3165'}\n",
      "word {'rank': '35', 'count': '2480', 'prob': '0.00258', 'cumulative': '0.17412', 'docs': '1395', 'word-length': '6.0000', 'coherence': '-1.7092', 'uniform_dist': '0.0136', 'corpus_dist': '-0.0011', 'token-doc-diff': '0.0003', 'exclusivity': '0.1355'}\n",
      "word {'rank': '36', 'count': '2468', 'prob': '0.00257', 'cumulative': '0.17669', 'docs': '1227', 'word-length': '3.0000', 'coherence': '-2.3317', 'uniform_dist': '0.0135', 'corpus_dist': '0.0041', 'token-doc-diff': '0.0001', 'exclusivity': '0.7738'}\n",
      "word {'rank': '37', 'count': '2467', 'prob': '0.00257', 'cumulative': '0.17925', 'docs': '1471', 'word-length': '5.0000', 'coherence': '-1.6486', 'uniform_dist': '0.0135', 'corpus_dist': '-0.0002', 'token-doc-diff': '0.0004', 'exclusivity': '0.1856'}\n",
      "word {'rank': '38', 'count': '2432', 'prob': '0.00253', 'cumulative': '0.18179', 'docs': '1060', 'word-length': '4.0000', 'coherence': '-2.3030', 'uniform_dist': '0.0133', 'corpus_dist': '0.0046', 'token-doc-diff': '0.0000', 'exclusivity': '0.8991'}\n",
      "word {'rank': '39', 'count': '2431', 'prob': '0.00253', 'cumulative': '0.18432', 'docs': '1342', 'word-length': '3.0000', 'coherence': '-2.2235', 'uniform_dist': '0.0133', 'corpus_dist': '0.0015', 'token-doc-diff': '0.0003', 'exclusivity': '0.3405'}\n",
      "word {'rank': '40', 'count': '2400', 'prob': '0.00250', 'cumulative': '0.18681', 'docs': '769', 'word-length': '5.0000', 'coherence': '-2.5716', 'uniform_dist': '0.0131', 'corpus_dist': '0.0031', 'token-doc-diff': '0.0000', 'exclusivity': '0.6170'}\n",
      "word {'rank': '41', 'count': '2377', 'prob': '0.00247', 'cumulative': '0.18929', 'docs': '1459', 'word-length': '5.0000', 'coherence': '-1.6614', 'uniform_dist': '0.0129', 'corpus_dist': '0.0014', 'token-doc-diff': '0.0004', 'exclusivity': '0.3298'}\n",
      "word {'rank': '42', 'count': '2349', 'prob': '0.00245', 'cumulative': '0.19173', 'docs': '1479', 'word-length': '4.0000', 'coherence': '-1.6835', 'uniform_dist': '0.0127', 'corpus_dist': '0.0020', 'token-doc-diff': '0.0005', 'exclusivity': '0.3983'}\n",
      "word {'rank': '43', 'count': '2324', 'prob': '0.00242', 'cumulative': '0.19415', 'docs': '1348', 'word-length': '5.0000', 'coherence': '-1.7926', 'uniform_dist': '0.0126', 'corpus_dist': '0.0030', 'token-doc-diff': '0.0003', 'exclusivity': '0.5796'}\n",
      "word {'rank': '44', 'count': '2323', 'prob': '0.00242', 'cumulative': '0.19657', 'docs': '1531', 'word-length': '3.0000', 'coherence': '-1.6222', 'uniform_dist': '0.0126', 'corpus_dist': '0.0007', 'token-doc-diff': '0.0006', 'exclusivity': '0.2553'}\n",
      "word {'rank': '45', 'count': '2283', 'prob': '0.00238', 'cumulative': '0.19895', 'docs': '712', 'word-length': '5.0000', 'coherence': '-3.2487', 'uniform_dist': '0.0123', 'corpus_dist': '0.0046', 'token-doc-diff': '0.0001', 'exclusivity': '0.9676'}\n",
      "word {'rank': '46', 'count': '2279', 'prob': '0.00237', 'cumulative': '0.20132', 'docs': '1070', 'word-length': '9.0000', 'coherence': '-2.2738', 'uniform_dist': '0.0123', 'corpus_dist': '0.0037', 'token-doc-diff': '0.0001', 'exclusivity': '0.7635'}\n",
      "word {'rank': '47', 'count': '2238', 'prob': '0.00233', 'cumulative': '0.20365', 'docs': '444', 'word-length': '5.0000', 'coherence': '-3.9873', 'uniform_dist': '0.0120', 'corpus_dist': '0.0046', 'token-doc-diff': '0.0004', 'exclusivity': '0.9958'}\n",
      "word {'rank': '48', 'count': '2226', 'prob': '0.00232', 'cumulative': '0.20597', 'docs': '1337', 'word-length': '4.0000', 'coherence': '-1.7849', 'uniform_dist': '0.0119', 'corpus_dist': '0.0024', 'token-doc-diff': '0.0004', 'exclusivity': '0.4898'}\n",
      "word {'rank': '49', 'count': '2192', 'prob': '0.00228', 'cumulative': '0.20825', 'docs': '1349', 'word-length': '4.0000', 'coherence': '-1.6734', 'uniform_dist': '0.0117', 'corpus_dist': '0.0013', 'token-doc-diff': '0.0004', 'exclusivity': '0.3114'}\n",
      "word {'rank': '50', 'count': '2177', 'prob': '0.00227', 'cumulative': '0.21051', 'docs': '1481', 'word-length': '4.0000', 'coherence': '-1.5161', 'uniform_dist': '0.0116', 'corpus_dist': '-0.0016', 'token-doc-diff': '0.0006', 'exclusivity': '0.1129'}\n"
     ]
    }
   ],
   "source": [
    "for child in root[0]:\n",
    "    print(child.tag, child.attrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rank': '1', 'count': '22666', 'prob': '0.02359', 'cumulative': '0.02359', 'docs': '3747', 'word-length': '3.0000', 'coherence': '0.0000', 'uniform_dist': '0.1762', 'corpus_dist': '0.0458', 'token-doc-diff': '0.0064', 'exclusivity': '0.9749'}\n",
      "3.0000\n"
     ]
    }
   ],
   "source": [
    "print(root[0][0].attrib)\n",
    "print(root[0][0].attrib['word-length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(root))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried to use the u-mass measure of topic coherence provided by MALLET to choose the optimal number of topics and the best value for opt interval. I trained several models on different settings of num-topics and opt-interval, and then made a table with the average coherence of the topics of each model to see which model had the highest average coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-92-ab0e6be24cc8>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-92-ab0e6be24cc8>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    $$c = \\sqrt{a^2 + b^2}$$\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import display, Math, Latex\n",
    "$$c = \\sqrt{a^2 + b^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#charmin = the minimum number of characters allowed in the input data\n",
    "#numtopics = the option num-topics on MALLET\n",
    "#optintverval = the option opt-interval on MALLET\n",
    "def getTopicTree(charmin, numtopics, optinterval, strip_legal):\n",
    "    if strip_legal:\n",
    "        path = 'ladvice_min{cm}_strip_legal/ladvice_min{cm}_numtopics{nt}/ladvice_min{cm}_numtopics{nt}_optinterval{oi}/ladvice_diagnostics_min{cm}_numtopics{nt}_optinterval{oi}.xml'.format(cm = charmin, nt = numtopics, oi = optinterval)\n",
    "    else:\n",
    "        path = 'ladvice_min{cm}/ladvice_min{cm}_numtopics{nt}/ladvice_min{cm}_numtopics{nt}_optinterval{oi}/ladvice_diagnostics_min{cm}_numtopics{nt}_optinterval{oi}.xml'.format(cm = charmin, nt = numtopics, oi = optinterval)\n",
    "    \n",
    "    try:\n",
    "        return ET.parse(path)\n",
    "    except FileNotFoundError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-7771.8171999999995, -7814.47984, -7878.86156],\n",
       " [-7981.034833333334, -8189.9647, -8094.2973833333335],\n",
       " [-8346.982314285715, -8299.994271428572, -8236.297271428573],\n",
       " [-8528.301800000001, -8547.317249999998, -8528.0237875],\n",
       " [-8676.977677777779, -8581.789477777778, -8785.028266666668],\n",
       " [-8923.557250000002, -8831.2263, -8914.755570000001],\n",
       " [-10068.403845, -10207.737894999998, -9970.474075],\n",
       " [-11334.458990000001, -11359.443075, -11254.0549575]]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_min_list = ['2000', '3000']\n",
    "num_topics_list = ['5','6','7','8','9','10','20','40']#,'60','80','14','18','20','40','60','80','100']\n",
    "opt_interval_list = ['10','50','100']\n",
    "\n",
    "coherences = {}\n",
    "for char_min in char_min_list:\n",
    "    table = []\n",
    "    for num_topics in num_topics_list:\n",
    "        row = []\n",
    "        for opt_interval in opt_interval_list:\n",
    "            tree = getTopicTree(char_min, num_topics, opt_interval, True)\n",
    "            if tree == 0:\n",
    "                row.append('----')\n",
    "                continue\n",
    "            root = tree.getroot()\n",
    "            coherence_sum = 0\n",
    "            for child in root:\n",
    "                coherence_sum = coherence_sum + float(child.attrib['coherence'])\n",
    "            row.append(coherence_sum/len(list(root)))\n",
    "        table.append(row)\n",
    "    coherences[char_min] = table\n",
    "    \n",
    "coherences['3000']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial observations of the coherence table:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average coherence increases (comes closer to 0) when the number of topics decreases. I feel like I am making a mistake while calculating the topic coherence, since it seems to so consistently increase when the topics are decreased. I tested this by training models on the extreme end (2-5 topics), and the coherence keeps increasing, even though 2 topics seems unreasonable to me. Until I figure out what the issue is, I will just compare the models at different numbers of topics myself and refer to the average coherence to examine models at different values of opt-interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Themes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations based on the top 50 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I looked at the topic keys for models with 5-10 topics with opt-interval at 10, 50, 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 topics:\n",
    "<ul>\n",
    "    <li>Money (Employment mostly)</li>\n",
    "    <li>Family (custody, divorce, etc.)</li>\n",
    "    <li>Crime </li>\n",
    "    <li>Property </li>\n",
    "    <li>Insurance/Damages</li>\n",
    "</ul>\n",
    "\n",
    "6 topics:\n",
    "<ul>\n",
    "    <li>Money (loans, debt, suing, services, payment)</li>\n",
    "    <li>Family (custody, divorce, etc.)</li>\n",
    "    <li>Crime</li>\n",
    "    <li>Employment </li>\n",
    "    <li>Property </li>\n",
    "    <li>Traffic Accidents</li>\n",
    "</ul>\n",
    "\n",
    "9 topics:\n",
    "<ul>\n",
    "    <li>Money (loans, debt, suing, services, payment)</li>\n",
    "    <li>Family (custody, divorce, etc.)</li>\n",
    "    <li>Crime (usually includes sexual violence in the top words)</li>\n",
    "    <li>Employment </li>\n",
    "    <li>Property </li>\n",
    "    <li>Renting </li>\n",
    "    <li>Traffic Accidents</li>\n",
    "    <li>Health</li>\n",
    "    <li>Variable new topic: sometimes split off from crime or property, sometimes an unclear topic with words related to school, copyright, and foreign countries</li>\n",
    "</ul>\n",
    "\n",
    "\n",
    "Areas of murky grouping:\n",
    "<ul>\n",
    "    <li>Traffic Accidents: With 5 topics and opt-interval = 50,100, and 6 topics opt-interval = 50, words related to car accidents are split between a topic about insurance and damages in general, and Crime. Most of the time the topic is specifically about traffic accidents. There is also some overlap between Traffic Accidents with Property (expected since accidents often cause damage to property). Words related to animals sometimes appear as top words in Traffic Accidents, and sometimes the top words of Traffic Accidents have a lot to do with property. </li><br> \n",
    "    <li>Health: With 5 topics, words related to healthcare appear under Family. With 6 topics, they appear under Employment, probably because in the US, health insurance is tied to employment. When I increase the number of topics to 7, it sometimes becomes a separate topic, but other times the model decides to split Property to create a separate topic for renting issues. After 8 topics, Health is usually a separate topic, sometimes focused on animals and vet care, sometimes on mental health issues.</li><br> \n",
    "    <li>Animals: Usually words relating to animals are under Property, sometimes they are under Traffic Accidents or Healthcare, and sometimes they are spread out. This is probably because animals can be involved in a lot of different situations. There may be issues with pets in leasing agreements, pets causing noise, pets could be going through medical procedures, etc. As for Traffic Accidents, it could be that cars running over pets is common enough for the model to sometimes group them together, or it could just be the result of the overlap between Traffic Accidents and Property. </li><br> \n",
    "    <li>School: Words related to school like \"students\", \"school\", \"class\", \"college\", \"university\" usually appear under Employment, Crime, or Family. Just like with Animals, I think this is because of how many different situations school could be involved in. People could be getting into academic trouble that could have academic implications like plagerism and cheating (Crime), students could be caught with drugs at school (Crime), separated parents could argue about how to arrange children's schooling (Family), university students could have problems with unemployment or discrimination (Employment). I am less clear on all the ways school is related to employment under the umbrella of legal issues, so I think I will examine that further when looking at models with more topics. <br> Words related to school also sometimes become part of a separate topic, though what this topic is about is unclear. It includes words related to websites, businesses, copyright, visas, foreign countries. I think it is some combination of copyright issues and immigration issues, school being the what causes them to overlap. Increasing to 10 topics does not make this topic clear, but I think after 11 or 12 topics it would split into two topics. </li><br> \n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
